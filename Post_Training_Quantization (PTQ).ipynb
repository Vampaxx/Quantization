{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307),(0.3081)),\n",
    "                    ])\n",
    "\n",
    "mnist_trainset  = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader    = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "mnist_testset   = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader     = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "  def __init__(self,hidden_size_1=100,hidden_size_2=100):\n",
    "    super(SimpleNet,self).__init__()\n",
    "    self.linear1  = nn.Linear(28*28,hidden_size_1)\n",
    "    self.linear2  = nn.Linear(hidden_size_1,hidden_size_2)\n",
    "    self.linear3  = nn.Linear(hidden_size_2,10)\n",
    "    self.relu     = nn.ReLU()\n",
    "\n",
    "  def forward(self,img):\n",
    "    x = img.reshape(-1,28*28)\n",
    "    x = self.relu(self.linear1(x))\n",
    "    x = self.relu(self.linear2(x))\n",
    "    x = self.linear3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (linear1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 100]          78,500\n",
      "              ReLU-2                  [-1, 100]               0\n",
      "            Linear-3                  [-1, 100]          10,100\n",
      "              ReLU-4                  [-1, 100]               0\n",
      "            Linear-5                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = SimpleNet().to(device)\n",
    "print(model)\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,model,epochs=5,total_iteration_limit=None):\n",
    "\n",
    "  cross_entro_loss  = nn.CrossEntropyLoss()\n",
    "  optimizer         = torch.optim.Adam(model.parameters(),lr=0.001) #Passing the parameters to an optimizer.\n",
    "  total_iteration   = 0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    loss_sum        = 0\n",
    "    num_iterations  = 0\n",
    "\n",
    "    data_iterator   = tqdm(train_loader, desc=f'Epoch  {epoch+1}')\n",
    "\n",
    "    if total_iteration_limit is not None:\n",
    "      data_iterator.total = total_iteration_limit\n",
    "\n",
    "    for data in data_iterator:\n",
    "      num_iterations  += 1\n",
    "      total_iteration += 1\n",
    "      image,label     = data\n",
    "      image           = image.to(device)\n",
    "      label           = label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output          = model(image.view(-1,28*28))\n",
    "      loss            = cross_entro_loss(output,label)\n",
    "      loss_sum       += loss.item()\n",
    "      avg_loss        = loss_sum / num_iterations\n",
    "      data_iterator.set_postfix(loss=avg_loss)          # update progress bar with loss\n",
    "      loss.backward()                                   # backward pass to calculate gradients\n",
    "      optimizer.step()                                  # Updates the model parameters using the computed gradients to minimize the loss.\n",
    "\n",
    "      if total_iteration_limit is not None and total_iteration >= total_iteration_limit:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'simplenet_ptq.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    model.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, model, epochs=2)\n",
    "    # Save the model to disk\n",
    "    torch.save(model.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (KB): 361.062\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **optimizer.zero_grad()**\n",
    "\n",
    "    - For every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropagation (i.e., updating the Weights and biases) because PyTorch accumulates the gradients on subsequent backward passes.\n",
    "\n",
    "    - This accumulating behavior is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "    \n",
    "    - when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient.\n",
    "    It would therefore point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "- **data_iterator.set_postfix()**\n",
    "\n",
    "\n",
    "    allows you to display additional information (in this case, the loss) next to the progress bar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight: 0.007502851076424122\n",
      "linear1.bias: -0.03206382691860199\n",
      "linear2.weight: -0.027867062017321587\n",
      "linear2.bias: -0.1109653189778328\n",
      "linear3.weight: -0.10847935825586319\n",
      "linear3.bias: -0.021669870242476463\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict():\n",
    "  print(f\"{key}: {model.state_dict().get(key).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module,total_iteration:int=None):\n",
    "  correct     = 0\n",
    "  total       = 0\n",
    "  iterations  = 0\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():                           # When evaluating the model on a validation set or performing inference on new data, you do not need gradient computations.\n",
    "    for data in tqdm(test_loader,desc=\"Testing\"):\n",
    "      image, label = data\n",
    "      image   = image.to(device)\n",
    "      label   = label.to(device)\n",
    "      output  = model(image.reshape(-1,784))\n",
    "\n",
    "      for index, i in enumerate(output):\n",
    "        if torch.argmax(i) == label[index]:\n",
    "          correct += 1\n",
    "        total +=1\n",
    "\n",
    "      iterations += 1\n",
    "      if total_iteration is not None and iterations >= total_iteration:\n",
    "        break\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print weights and size of the model before quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight before quantization\n",
      "Parameter containing:\n",
      "tensor([[ 0.0393, -0.0006,  0.0137,  ...,  0.0013, -0.0181,  0.0328],\n",
      "        [-0.0207,  0.0032,  0.0330,  ..., -0.0303, -0.0180,  0.0186],\n",
      "        [ 0.0638,  0.1237,  0.0910,  ...,  0.0883,  0.1152,  0.1136],\n",
      "        ...,\n",
      "        [ 0.0175, -0.0349, -0.0273,  ..., -0.0308, -0.0263, -0.0293],\n",
      "        [ 0.0780,  0.0357,  0.0261,  ...,  0.0798,  0.0573,  0.0307],\n",
      "        [ 0.0030,  0.0069,  0.0268,  ...,  0.0027, -0.0172,  0.0402]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "torch.float32\n",
      "Size (KB): 361.062\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight before quantization\")\n",
    "print(model.linear1.weight)\n",
    "print(model.linear1.weight.dtype)\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:02<00:00, 382.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model before quantization: ')\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Min-Max Observation in the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedVerySimpleNet(\n",
      "  (quantize): QuantStub()\n",
      "  (linear1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dequantize): DeQuantStub()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         QuantStub-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 100]          78,500\n",
      "              ReLU-3                  [-1, 100]               0\n",
      "            Linear-4                  [-1, 100]          10,100\n",
      "              ReLU-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 10]           1,010\n",
      "       DeQuantStub-7                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class QuantizedVerySimpleNet(nn.Module):\n",
    "\n",
    "  def __init__(self,hidden_size_1=100,hidden_size_2=100):\n",
    "    super(QuantizedVerySimpleNet,self).__init__()\n",
    "    self.quantize   = torch.quantization.QuantStub()\n",
    "    self.linear1    = nn.Linear(28*28,hidden_size_1)\n",
    "    self.linear2    = nn.Linear(hidden_size_1,hidden_size_2)\n",
    "    self.linear3    = nn.Linear(hidden_size_2,10)\n",
    "    self.relu       = nn.ReLU()\n",
    "    self.dequantize = torch.quantization.DeQuantStub()\n",
    "\n",
    "  def forward(self,img):\n",
    "    x = img.reshape(-1,(28*28))\n",
    "    x = self.quantize(x)\n",
    "    x = self.relu(self.linear1(x))\n",
    "    x = self.relu(self.linear2(x))\n",
    "    x = self.linear3(x)\n",
    "    x = self.dequantize(x)\n",
    "    return x\n",
    "\n",
    "quantize_model = QuantizedVerySimpleNet().to(device)\n",
    "print(quantize_model)\n",
    "summary(quantize_model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quantize): QuantStub()\n",
       "  (linear1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dequantize): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # copy the weight from unquntized model\n",
    "quantize_model.load_state_dict(model.state_dict())\n",
    "quantize_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quantize): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequantize): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_model.qconfig  = torch.ao.quantization.default_qconfig         # set default quntize\n",
    "quantize_model          = torch.ao.quantization.prepare(quantize_model) # Insert observers\n",
    "quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:03<00:00, 256.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(quantize_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:04<00:00, 235.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(quantize_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quantize): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-86.30210876464844, max_val=53.823486328125)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-83.1438980102539, max_val=57.77360534667969)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-125.01261901855469, max_val=38.11832046508789)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequantize): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model using the statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quantize): Quantize(scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([17], device='cuda:0'), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=1.10335111618042, zero_point=78, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=1.1095867156982422, zero_point=75, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=1.2844955921173096, zero_point=97, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequantize): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_model_c = torch.ao.quantization.convert(quantize_model)\n",
    "\n",
    "print(f'Check statistics of the various layers')\n",
    "quantize_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimpleNet(\n",
       "  (quantize): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-86.30210876464844, max_val=53.823486328125)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-83.1438980102539, max_val=57.77360534667969)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-125.01261901855469, max_val=38.11832046508789)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequantize): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print weights of the model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[ 4,  0,  1,  ...,  0, -2,  3],\n",
      "        [-2,  0,  3,  ..., -3, -2,  2],\n",
      "        [ 6, 12,  9,  ...,  9, 11, 11],\n",
      "        ...,\n",
      "        [ 2, -3, -3,  ..., -3, -3, -3],\n",
      "        [ 8,  3,  3,  ...,  8,  6,  3],\n",
      "        [ 0,  1,  3,  ...,  0, -2,  4]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(torch.int_repr(quantize_model.linear1.weight()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the dequantized weights and the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[ 0.0393, -0.0006,  0.0137,  ...,  0.0013, -0.0181,  0.0328],\n",
      "        [-0.0207,  0.0032,  0.0330,  ..., -0.0303, -0.0180,  0.0186],\n",
      "        [ 0.0638,  0.1237,  0.0910,  ...,  0.0883,  0.1152,  0.1136],\n",
      "        ...,\n",
      "        [ 0.0175, -0.0349, -0.0273,  ..., -0.0308, -0.0263, -0.0293],\n",
      "        [ 0.0780,  0.0357,  0.0261,  ...,  0.0798,  0.0573,  0.0307],\n",
      "        [ 0.0030,  0.0069,  0.0268,  ...,  0.0027, -0.0172,  0.0402]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[ 0.0409,  0.0000,  0.0102,  ...,  0.0000, -0.0204,  0.0307],\n",
      "        [-0.0204,  0.0000,  0.0307,  ..., -0.0307, -0.0204,  0.0204],\n",
      "        [ 0.0613,  0.1226,  0.0920,  ...,  0.0920,  0.1124,  0.1124],\n",
      "        ...,\n",
      "        [ 0.0204, -0.0307, -0.0307,  ..., -0.0307, -0.0307, -0.0307],\n",
      "        [ 0.0817,  0.0307,  0.0307,  ...,  0.0817,  0.0613,  0.0307],\n",
      "        [ 0.0000,  0.0102,  0.0307,  ...,  0.0000, -0.0204,  0.0409]],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(model.linear1.weight)\n",
    "print('')\n",
    "print(f'Dequantized weights: ')\n",
    "print(torch.dequantize(quantize_model.linear1.weight()))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (KB): 95.458\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(quantize_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tensor = [value.data for key,value in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0393, -0.0006,  0.0137,  ...,  0.0013, -0.0181,  0.0328],\n",
       "        [-0.0207,  0.0032,  0.0330,  ..., -0.0303, -0.0180,  0.0186],\n",
       "        [ 0.0638,  0.1237,  0.0910,  ...,  0.0883,  0.1152,  0.1136],\n",
       "        ...,\n",
       "        [ 0.0175, -0.0349, -0.0273,  ..., -0.0308, -0.0263, -0.0293],\n",
       "        [ 0.0780,  0.0357,  0.0261,  ...,  0.0798,  0.0573,  0.0307],\n",
       "        [ 0.0030,  0.0069,  0.0268,  ...,  0.0027, -0.0172,  0.0402]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear1_weight = quantize_model.linear1.weight().dequantize().detach().cpu()\n",
    "model_tensor[0].detach().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print size and accuracy of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 364.834\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(quantize_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:04<00:00, 226.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(quantize_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
